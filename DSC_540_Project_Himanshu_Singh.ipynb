{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Preperation\n",
    "### Term Project - MileStone 2\n",
    "### Submitter - Himanshu Singh\n",
    "### Cleaning/Formatting Flat File Source"
   ],
   "id": "d8a5304832f334de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:03.087989Z",
     "start_time": "2025-11-13T06:22:02.902422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We got the Latitude and Longitude level info in covid_dataset. It also has data from Jan 22 to March 24 2021.\n",
    "# We got now Weekly Covid Cases and Weekly Covid Deaths information from https://ourworldindata.org/search?q=covid+data\n",
    "# We will try to merge the information from different dataset and come up with updated Covid cases nd Covid Death info country wise.\n",
    "\n",
    "\n",
    "# Loading the different data frames\n",
    "import pandas as pd\n",
    "from pandas.conftest import ordered\n",
    "\n",
    "# Define the path to your .csv file\n",
    "file_path = 'covid_dataset.csv'\n",
    "file_path_cases = 'weekly-covid-cases.csv'\n",
    "file_path_death = 'weekly-covid-deaths.csv'\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "try:\n",
    "    # Reading the file\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_cases = pd.read_csv(file_path_cases)\n",
    "    df_death = pd.read_csv(file_path_death)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    # Reading first 10 rows\n",
    "    #print(df.head(10)) # Print the first 10 rows of the DataFrame\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd4ef3eaaa776af2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T21:43:15.498744Z",
     "start_time": "2025-10-19T21:43:15.340675Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Step 1 To check on the dataframe quality and check on Outliers\n",
   "id": "ef0e981ec49bf5ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:03.173088Z",
     "start_time": "2025-11-13T06:22:03.094850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In order to get fresh data and we need to update our data frame\n",
    "# We will merge the cases reported and Deaths in One data Frame\n",
    "\n",
    "# Lets start with cleaning up the dataframes of Death and Covid Cases\n",
    "# The Dataframe.describe() shows data in Exponential so changing that\n",
    "pd.set_option('display.float_format', '{:f}'.format)\n",
    "\n",
    "# Checking the Range of the data to see if there are outliers\n",
    "print(df_death.describe())\n",
    "\n",
    "# Max No of Deaths is about 100,000 which is seems to be correct and doens't seems to be an Outlier\n",
    "\n",
    "print(df_cases.describe())\n",
    "\n",
    "# On seeing case data it seems to be an outlier. We will examine the data and see if that is required or is an outlier\n",
    "\n",
    "\n",
    "# --- 1. Sort the DataFrame ---\n",
    "# Sort by 'Score' in descending order (highest score first)\n",
    "df_sorted = df_cases.sort_values(by='Weekly cases', ascending=False)\n",
    "\n",
    "# --- 2. Select the top 10 rows ---\n",
    "# Use the .head(10) method on the sorted DataFrame\n",
    "top_10_rows = df_sorted.head(10)\n",
    "print (top_10_rows)\n",
    "\n",
    "# The data seems to be a consolidated number , of World, which is not required for our project so we can eliminate it.\n",
    "# Also we found the data contains some continent level info which will not be required for our analysis and could be eliminated\n",
    "\n",
    "# All World level and continent and group of countries information has to be eliminated.\n",
    "\n",
    "unique_value =df_cases['Entity'].unique()\n",
    "print(unique_value)\n",
    "\n",
    "values_to_drop = ['World','World excl. China','World excl. China and South Korea','World excl. China, South Korea, Japan and Singapore','Upper-middle-income countries','Oceania','North America','South America','Lower-middle-income countries','Low-income countries','High-income countries','Europe','European Union (27)','Asia','Asia excl. China','Africa']\n",
    "\n",
    "df_cases = df_cases[~df_cases['Entity'].isin(values_to_drop)]\n",
    "df_death = df_death[~df_death['Entity'].isin(values_to_drop)]\n"
   ],
   "id": "1874165998dd21e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Weekly deaths\n",
      "count  517936.000000\n",
      "mean      703.079655\n",
      "std      5172.164055\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%        13.000000\n",
      "max    103745.000000\n",
      "         Weekly cases\n",
      "count   517028.000000\n",
      "mean     73585.883828\n",
      "std     699715.312594\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%         31.000000\n",
      "75%       1353.000000\n",
      "max   44814230.000000\n",
      "                               Entity      Code         Day  Weekly cases\n",
      "503481                          World  OWID_WRL  2022-12-26      44814230\n",
      "503482                          World  OWID_WRL  2022-12-27      44236932\n",
      "503480                          World  OWID_WRL  2022-12-25      44236205\n",
      "26160                            Asia       NaN  2022-12-26      42768774\n",
      "503483                          World  OWID_WRL  2022-12-28      42712671\n",
      "26161                            Asia       NaN  2022-12-27      42222120\n",
      "26159                            Asia       NaN  2022-12-25      42142424\n",
      "503479                          World  OWID_WRL  2022-12-24      41841003\n",
      "486763  Upper-middle-income countries       NaN  2022-12-26      41595420\n",
      "95125                           China       CHN  2022-12-26      41174900\n",
      "['Afghanistan' 'Africa' 'Albania' 'Algeria' 'American Samoa' 'Andorra'\n",
      " 'Angola' 'Anguilla' 'Antigua and Barbuda' 'Argentina' 'Armenia' 'Aruba'\n",
      " 'Asia' 'Asia excl. China' 'Australia' 'Austria' 'Azerbaijan' 'Bahamas'\n",
      " 'Bahrain' 'Bangladesh' 'Barbados' 'Belarus' 'Belgium' 'Belize' 'Benin'\n",
      " 'Bermuda' 'Bhutan' 'Bolivia' 'Bonaire Sint Eustatius and Saba'\n",
      " 'Bosnia and Herzegovina' 'Botswana' 'Brazil' 'British Virgin Islands'\n",
      " 'Brunei' 'Bulgaria' 'Burkina Faso' 'Burundi' 'Cambodia' 'Cameroon'\n",
      " 'Canada' 'Cape Verde' 'Cayman Islands' 'Central African Republic' 'Chad'\n",
      " 'Chile' 'China' 'Colombia' 'Comoros' 'Congo' 'Cook Islands' 'Costa Rica'\n",
      " 'Croatia' 'Cuba' 'Curacao' 'Cyprus' 'Czechia'\n",
      " 'Democratic Republic of Congo' 'Denmark' 'Djibouti' 'Dominica'\n",
      " 'Dominican Republic' 'East Timor' 'Ecuador' 'Egypt' 'El Salvador'\n",
      " 'Equatorial Guinea' 'Eritrea' 'Estonia' 'Eswatini' 'Ethiopia' 'Europe'\n",
      " 'European Union (27)' 'Falkland Islands' 'Faroe Islands' 'Fiji' 'Finland'\n",
      " 'France' 'French Guiana' 'French Polynesia' 'Gabon' 'Gambia' 'Georgia'\n",
      " 'Germany' 'Ghana' 'Gibraltar' 'Greece' 'Greenland' 'Grenada' 'Guadeloupe'\n",
      " 'Guam' 'Guatemala' 'Guernsey' 'Guinea' 'Guinea-Bissau' 'Guyana' 'Haiti'\n",
      " 'High-income countries' 'Honduras' 'Hungary' 'Iceland' 'India'\n",
      " 'Indonesia' 'Iran' 'Iraq' 'Ireland' 'Isle of Man' 'Israel' 'Italy'\n",
      " 'Jamaica' 'Japan' 'Jersey' 'Jordan' 'Kazakhstan' 'Kenya' 'Kiribati'\n",
      " 'Kosovo' 'Kuwait' 'Kyrgyzstan' 'Laos' 'Latvia' 'Lebanon' 'Lesotho'\n",
      " 'Liberia' 'Libya' 'Liechtenstein' 'Lithuania' 'Low-income countries'\n",
      " 'Lower-middle-income countries' 'Luxembourg' 'Madagascar' 'Malawi'\n",
      " 'Malaysia' 'Maldives' 'Mali' 'Malta' 'Marshall Islands' 'Martinique'\n",
      " 'Mauritania' 'Mauritius' 'Mayotte' 'Mexico' 'Micronesia (country)'\n",
      " 'Moldova' 'Monaco' 'Mongolia' 'Montenegro' 'Montserrat' 'Morocco'\n",
      " 'Mozambique' 'Myanmar' 'Namibia' 'Nauru' 'Nepal' 'Netherlands'\n",
      " 'New Caledonia' 'New Zealand' 'Nicaragua' 'Niger' 'Nigeria' 'Niue'\n",
      " 'North America' 'North Korea' 'North Macedonia'\n",
      " 'Northern Mariana Islands' 'Norway' 'Oceania' 'Oman' 'Pakistan' 'Palau'\n",
      " 'Palestine' 'Panama' 'Papua New Guinea' 'Paraguay' 'Peru' 'Philippines'\n",
      " 'Pitcairn' 'Poland' 'Portugal' 'Puerto Rico' 'Qatar' 'Reunion' 'Romania'\n",
      " 'Russia' 'Rwanda' 'Saint Barthelemy' 'Saint Helena'\n",
      " 'Saint Kitts and Nevis' 'Saint Lucia' 'Saint Martin (French part)'\n",
      " 'Saint Pierre and Miquelon' 'Saint Vincent and the Grenadines' 'Samoa'\n",
      " 'San Marino' 'Sao Tome and Principe' 'Saudi Arabia' 'Senegal' 'Serbia'\n",
      " 'Seychelles' 'Sierra Leone' 'Singapore' 'Sint Maarten (Dutch part)'\n",
      " 'Slovakia' 'Slovenia' 'Solomon Islands' 'Somalia' 'South Africa'\n",
      " 'South America' 'South Korea' 'South Sudan' 'Spain' 'Sri Lanka' 'Sudan'\n",
      " 'Suriname' 'Sweden' 'Switzerland' 'Syria' 'Tajikistan' 'Tanzania'\n",
      " 'Thailand' 'Togo' 'Tokelau' 'Tonga' 'Trinidad and Tobago' 'Tunisia'\n",
      " 'Turkey' 'Turkmenistan' 'Turks and Caicos Islands' 'Tuvalu' 'Uganda'\n",
      " 'Ukraine' 'United Arab Emirates' 'United Kingdom' 'United States'\n",
      " 'United States Virgin Islands' 'Upper-middle-income countries' 'Uruguay'\n",
      " 'Uzbekistan' 'Vanuatu' 'Vatican' 'Venezuela' 'Vietnam'\n",
      " 'Wallis and Futuna' 'World' 'World excl. China'\n",
      " 'World excl. China and South Korea'\n",
      " 'World excl. China, South Korea, Japan and Singapore' 'Yemen' 'Zambia'\n",
      " 'Zimbabwe']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T21:57:22.974733Z",
     "start_time": "2025-10-19T21:57:22.568903Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Step 2 To check on the dataframe duplicates and NaN\n",
   "id": "500c9cfef443bdf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:03.374675Z",
     "start_time": "2025-11-13T06:22:03.177154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- Code to count duplicates ---\n",
    "duplicate_count_cases = df_cases.duplicated().sum()\n",
    "print(f\"Total number of duplicate rows found: {duplicate_count_cases}\")\n",
    "\n",
    "# --- Code to count duplicates ---\n",
    "duplicate_death_cases = df_death.duplicated().sum()\n",
    "print(f\"Total number of duplicate rows found: {duplicate_death_cases}\")\n",
    "\n",
    "# No Duplicates in the DataFrame\n",
    "#df_death.drop_duplicates()\n",
    "#df_cases.drop_duplicates()\n",
    "\n",
    "# --- Code to count NaN ---\n",
    "null_count_cases = df_cases.isna().sum().sum()\n",
    "print(f\"Total number of Null rows found: {null_count_cases}\")\n",
    "\n",
    "# --- Code to count NaN ---\n",
    "null_death_cases = df_death.isna().sum().sum()\n",
    "print(f\"Total number of Null rows found: {null_death_cases}\")\n",
    "\n",
    "# Checking the Rows having NaN\n",
    "null_rows_any_cases = df_cases[df_cases.isnull().any(axis=1)]\n",
    "print(null_rows_any_cases)\n",
    "\n",
    "# Checking the Rows having NaN\n",
    "null_rows_any_deaths = df_death[df_death.isnull().any(axis=1)]\n",
    "print(null_rows_any_deaths)\n",
    "\n",
    "\n",
    "\n",
    "# On Checking the data it is found that there are rows which has Africa continent info as well as Asia except China as different categories. For our analysis we will only take countries and excluded continent and non countries info\n",
    "# drop NaN from data frame\n",
    "df_cases.dropna(inplace=True)\n",
    "df_death.dropna(inplace=True)\n",
    "\n",
    "print(df_cases.head(10))\n",
    "print(df_death.head(10))\n",
    "\n"
   ],
   "id": "96e2802e44b53005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate rows found: 0\n",
      "Total number of duplicate rows found: 0\n",
      "Total number of Null rows found: 0\n",
      "Total number of Null rows found: 0\n",
      "Empty DataFrame\n",
      "Columns: [Entity, Code, Day, Weekly cases]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Entity, Code, Day, Weekly deaths]\n",
      "Index: []\n",
      "        Entity Code         Day  Weekly cases\n",
      "0  Afghanistan  AFG  2020-01-09             0\n",
      "1  Afghanistan  AFG  2020-01-10             0\n",
      "2  Afghanistan  AFG  2020-01-11             0\n",
      "3  Afghanistan  AFG  2020-01-12             0\n",
      "4  Afghanistan  AFG  2020-01-13             0\n",
      "5  Afghanistan  AFG  2020-01-14             0\n",
      "6  Afghanistan  AFG  2020-01-15             0\n",
      "7  Afghanistan  AFG  2020-01-16             0\n",
      "8  Afghanistan  AFG  2020-01-17             0\n",
      "9  Afghanistan  AFG  2020-01-18             0\n",
      "        Entity Code         Day  Weekly deaths\n",
      "0  Afghanistan  AFG  2020-01-09              0\n",
      "1  Afghanistan  AFG  2020-01-10              0\n",
      "2  Afghanistan  AFG  2020-01-11              0\n",
      "3  Afghanistan  AFG  2020-01-12              0\n",
      "4  Afghanistan  AFG  2020-01-13              0\n",
      "5  Afghanistan  AFG  2020-01-14              0\n",
      "6  Afghanistan  AFG  2020-01-15              0\n",
      "7  Afghanistan  AFG  2020-01-16              0\n",
      "8  Afghanistan  AFG  2020-01-17              0\n",
      "9  Afghanistan  AFG  2020-01-18              0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T21:57:22.974733Z",
     "start_time": "2025-10-19T21:57:22.568903Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Step 3 To reduce the data frame size by reporting cases and deaths Monthly instead of Daily. and merge the two Data Frames",
   "id": "9f09f7e37b6c1d1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:03.541761Z",
     "start_time": "2025-11-13T06:22:03.377487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Inorder to reduce the records, we will sum the data month wise for each data frame\n",
    "df_cases['Day'] = pd.to_datetime(df_cases['Day'])\n",
    "\n",
    "# Extract Year and Month into new columns\n",
    "df_cases['Year'] = df_cases['Day'].dt.year\n",
    "df_cases['Month'] = df_cases['Day'].dt.month\n",
    "\n",
    "# Inorder to reduce the records, we will sum the data month wise for each data frame\n",
    "df_death['Day'] = pd.to_datetime(df_death['Day'])\n",
    "\n",
    "# Extract Year and Month into new columns\n",
    "df_death['Year'] = df_death['Day'].dt.year\n",
    "df_death['Month'] = df_death['Day'].dt.month\n",
    "\n",
    "# Define the list of columns to group by\n",
    "grouping_keys = ['Year', 'Month', 'Code', 'Entity']\n",
    "\n",
    "df_grouped_deaths = df_death.groupby(grouping_keys)['Weekly deaths'].sum().reset_index()\n",
    "df_grouped_cases = df_cases.groupby(grouping_keys)['Weekly cases'].sum().reset_index()\n",
    "\n",
    "df_merged_same_1 = pd.merge(\n",
    "    df_grouped_deaths,\n",
    "    df_grouped_cases,\n",
    "    on=['Code', 'Year','Month', 'Entity'],\n",
    "    how='inner'\n",
    ")\n",
    "df_merged_same_1= df_merged_same_1.sort_values(by=['Entity'])\n",
    "\n",
    "# We want to report the data as on start of each month and later the weather will be pulled from API for start of each month.\n",
    "# Using Month and Year will form the start of the month\n",
    "df_merged_same_1['Date_String'] = (\n",
    "    df_merged_same_1['Year'].astype(str) + '-' +\n",
    "    df_merged_same_1['Month'].astype(str).str.zfill(2) + '-01'\n",
    ")\n",
    "\n",
    "df_merged_same_1['New_Date'] = pd.to_datetime(df_merged_same_1['Date_String'])\n",
    "df_merged_same_1.info()\n",
    "df_merged_same_1.to_csv('output_data1.csv', index=False)\n",
    "df_information=df_merged_same_1\n"
   ],
   "id": "55027180aeca377e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15971 entries, 4195 to 15970\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Year           15971 non-null  int32         \n",
      " 1   Month          15971 non-null  int32         \n",
      " 2   Code           15971 non-null  object        \n",
      " 3   Entity         15971 non-null  object        \n",
      " 4   Weekly deaths  15971 non-null  int64         \n",
      " 5   Weekly cases   15971 non-null  int64         \n",
      " 6   Date_String    15971 non-null  object        \n",
      " 7   New_Date       15971 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int32(2), int64(2), object(3)\n",
      "memory usage: 998.2+ KB\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T17:24:44.727375Z",
     "start_time": "2025-10-19T17:24:44.554356Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Step 4 - Get the Primary Data frame, remove duplicates and countries with state specific information. Only add the state national capital region for big countries which had state level informations",
   "id": "13de42c70582a19d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:03.557874Z",
     "start_time": "2025-11-13T06:22:03.545192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df_multi_dropped = df.drop(columns=['day', 'temperature','humidity','confirmed','deaths','recovered','quarantine', 'restrictions','schools'])\n",
    "df_multi_dropped = df_multi_dropped.drop_duplicates()\n",
    "df_multi_dropped.head(10)\n",
    "column_name = 'Country/Region'\n",
    "# Country/Region\n",
    "\n",
    "duplicate_rows = df_multi_dropped[df_multi_dropped[column_name].duplicated(keep=False)]\n",
    "\n",
    "print(f\"\\n--- All Rows Containing Duplicated Values in '{column_name}' ---\")\n",
    "print(duplicate_rows.sort_values(by=column_name))\n",
    "\n",
    "\n",
    "\n",
    "# ON analysis it is found that Australia, Canada, USA and China has state level info in the Data frame, SO we will take only one Geaographical Record for the Country (their Capital)\n",
    "\n",
    "\n",
    "province_to_select =['Australian Capital Territory','Maryland','Beijing','Ontario']\n",
    "\n",
    "# Condition: Select rows where 'Country/Region' is above Province list\n",
    "big_countries_data = df_multi_dropped[df_multi_dropped['Province/State'].isin(province_to_select)]\n",
    "\n",
    "# dropping multiple rows of the states of four countries\n",
    "values_to_drop = ['Australia','China','US','Canada']\n",
    "\n",
    "df_multi_dropped = df_multi_dropped[~df_multi_dropped['Country/Region'].isin(values_to_drop)]\n",
    "df_multi_dropped.drop_duplicates().to_csv('output_data.csv', index=False)\n",
    "\n",
    "column_to_check = 'Province/State'\n",
    "df_not_null = df_multi_dropped[df_multi_dropped[column_to_check].notna()]\n",
    "print(df_not_null)\n",
    "\n",
    "# There are few places like Greenland which shows under Denmark, Correcting those to come under Countries instead of Provinces\n",
    "\n",
    "df_multi_dropped['Country/Region'] = df_multi_dropped['Province/State'].where(\n",
    "    df_multi_dropped['Province/State'].notna(), # Condition: Keep value if Target_Column is NOT NaN\n",
    "    other=df_multi_dropped['Country/Region']    # Else (if it IS NaN), replace with Source_Column\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#print(big_countries_data.tail(15))\n",
    "\n",
    "\n",
    "#df_multi_dropped.drop_duplicates().to_csv('output_data.csv', index=False)\n",
    "\n",
    "df_master= df_multi_dropped.drop_duplicates()\n",
    "df_master = pd.concat([df_multi_dropped, big_countries_data], ignore_index=True)\n",
    "print(df_master.head(15))\n",
    "df_master.drop_duplicates().to_csv('output_data.csv', index=False)\n"
   ],
   "id": "3c7a682a81531662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Rows Containing Duplicated Values in 'Country/Region' ---\n",
      "                     Province/State  Country/Region        lat       long  \\\n",
      "488    Australian Capital Territory       Australia -35.473500 149.012400   \n",
      "549           From Diamond Princess       Australia  35.443700 139.638000   \n",
      "610                 New South Wales       Australia -33.868800 151.209300   \n",
      "671              Northern Territory       Australia -12.463400 130.845600   \n",
      "732                      Queensland       Australia -28.016700 153.400000   \n",
      "...                             ...             ...        ...        ...   \n",
      "17568                    Montserrat  United Kingdom  16.742500 -62.187400   \n",
      "17324                Cayman Islands  United Kingdom  19.313300 -81.254600   \n",
      "17263                       Bermuda  United Kingdom  32.307800 -64.750500   \n",
      "17385               Channel Islands  United Kingdom  49.372300  -2.364400   \n",
      "17629                United Kingdom  United Kingdom  55.378100  -3.436000   \n",
      "\n",
      "            pop  urbanpop  density  medianage         smokers  tests  \\\n",
      "488    25499884  21929900        3         38  3799482.716000  31635   \n",
      "549    25499884  21929900        3         38  3799482.716000  31635   \n",
      "610    25499884  21929900        3         38  3799482.716000  31635   \n",
      "671    25499884  21929900        3         38  3799482.716000  31635   \n",
      "732    25499884  21929900        3         38  3799482.716000  31635   \n",
      "...         ...       ...      ...        ...             ...    ...   \n",
      "17568  67886011  56345389      281         40 13000171.110000  50442   \n",
      "17324  67886011  56345389      281         40 13000171.110000  50442   \n",
      "17263  67886011  56345389      281         40 13000171.110000  50442   \n",
      "17385  67886011  56345389      281         40 13000171.110000  50442   \n",
      "17629  67886011  56345389      281         40 13000171.110000  50442   \n",
      "\n",
      "          testpop  health_exp_pc  hospibed  \n",
      "488    806.065560    4934.000000  3.800000  \n",
      "549    806.065560    4934.000000  3.800000  \n",
      "610    806.065560    4934.000000  3.800000  \n",
      "671    806.065560    4934.000000  3.800000  \n",
      "732    806.065560    4934.000000  3.800000  \n",
      "...           ...            ...       ...  \n",
      "17568 1345.823143    4356.000000  2.800000  \n",
      "17324 1345.823143    4356.000000  2.800000  \n",
      "17263 1345.823143    4356.000000  2.800000  \n",
      "17385 1345.823143    4356.000000  2.800000  \n",
      "17629 1345.823143    4356.000000  2.800000  \n",
      "\n",
      "[133 rows x 13 columns]\n",
      "         Province/State  Country/Region        lat       long       pop  \\\n",
      "5490   Diamond Princess     Cruise Ship  35.443700 139.638000      3500   \n",
      "5734            Denmark         Denmark  56.263900   9.501800   5792202   \n",
      "5795      Faroe Islands         Denmark  61.892600  -6.911800   5792202   \n",
      "5856          Greenland         Denmark  71.706900 -42.604300     56770   \n",
      "6771             France          France  46.227600   2.213700  65273511   \n",
      "6832      French Guiana          France   3.933900 -53.125800    298682   \n",
      "6893   French Polynesia          France -17.679700 149.406800  65273511   \n",
      "6954         Guadeloupe          France  16.250000 -61.583300    400124   \n",
      "7015            Mayotte          France -12.827500  45.166200    272815   \n",
      "7076      New Caledonia          France -20.904300 165.618000  65273511   \n",
      "7137            Reunion          France -21.135100  55.247100    895312   \n",
      "7198   Saint Barthelemy          France  17.900000 -62.833300  65273511   \n",
      "7259          St Martin          France  18.070800 -63.050100  65273511   \n",
      "10431             Aruba     Netherlands  12.518600 -70.035800    106766   \n",
      "10492           Curacao     Netherlands  12.169600 -68.990000  17134872   \n",
      "10553       Netherlands     Netherlands  52.132600   5.291300  17134872   \n",
      "10614      Sint Maarten     Netherlands  18.042500 -63.054800  17134872   \n",
      "17263           Bermuda  United Kingdom  32.307800 -64.750500  67886011   \n",
      "17324    Cayman Islands  United Kingdom  19.313300 -81.254600  67886011   \n",
      "17385   Channel Islands  United Kingdom  49.372300  -2.364400  67886011   \n",
      "17446         Gibraltar  United Kingdom  36.140800  -5.353600  67886011   \n",
      "17507       Isle of Man  United Kingdom  54.236100  -4.548100  67886011   \n",
      "17568        Montserrat  United Kingdom  16.742500 -62.187400  67886011   \n",
      "17629    United Kingdom  United Kingdom  55.378100  -3.436000  67886011   \n",
      "\n",
      "       urbanpop  density  medianage         smokers  tests     testpop  \\\n",
      "5490       3500       25         62     -999.000000   -999 -999.000000   \n",
      "5734    5097137      137         42   984674.340000   7620  760.131496   \n",
      "5795    5097137      137         42   984674.340000   7620  760.131496   \n",
      "5856      49389        0         34     -999.000000   -999 -999.000000   \n",
      "6771   53524279      119         42 18080762.550000  11071 5895.900190   \n",
      "6832     259853        4         25     -999.000000   -999 -999.000000   \n",
      "6893   53524279      119         42 18080762.550000  11071 5895.900190   \n",
      "6954     384119      237         44     -999.000000   -999 -999.000000   \n",
      "7015     125494      728         20     -999.000000   -999 -999.000000   \n",
      "7076   53524279      119         42 18080762.550000  11071 5895.900190   \n",
      "7137     895312      358         36     -999.000000   -999 -999.000000   \n",
      "7198   53524279      119         42 18080762.550000  11071 5895.900190   \n",
      "7259   53524279      119         42 18080762.550000  11071 5895.900190   \n",
      "10431     46977      593         41     -999.000000   -999 -999.000000   \n",
      "10492  15764082      508         43  4292285.436000   6000 2855.812000   \n",
      "10553  15764082      508         43  4292285.436000   6000 2855.812000   \n",
      "10614  15764082      508         43  4292285.436000   6000 2855.812000   \n",
      "17263  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17324  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17385  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17446  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17507  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17568  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "17629  56345389      281         40 13000171.110000  50442 1345.823143   \n",
      "\n",
      "       health_exp_pc  hospibed  \n",
      "5490             NaN 13.400000  \n",
      "5734     5497.000000  2.500000  \n",
      "5795     5497.000000  2.500000  \n",
      "5856     5497.000000 14.353400  \n",
      "6771     4026.000000  6.500000  \n",
      "6832     4026.000000  3.200000  \n",
      "6893     4026.000000  6.500000  \n",
      "6954     4026.000000  0.700000  \n",
      "7015     4026.000000  7.000000  \n",
      "7076     4026.000000  6.500000  \n",
      "7137     4026.000000 10.200000  \n",
      "7198     4026.000000  6.500000  \n",
      "7259     4026.000000  6.500000  \n",
      "10431    4746.000000  1.200000  \n",
      "10492    4746.000000  4.700000  \n",
      "10553    4746.000000  4.700000  \n",
      "10614    4746.000000  4.700000  \n",
      "17263    4356.000000  2.800000  \n",
      "17324    4356.000000  2.800000  \n",
      "17385    4356.000000  2.800000  \n",
      "17446    4356.000000  2.800000  \n",
      "17507    4356.000000  2.800000  \n",
      "17568    4356.000000  2.800000  \n",
      "17629    4356.000000  2.800000  \n",
      "   Province/State       Country/Region        lat       long        pop  \\\n",
      "0             NaN          Afghanistan  33.000000  65.000000   38928346   \n",
      "1             NaN              Albania  41.153300  20.168300    2877797   \n",
      "2             NaN              Algeria  28.033900   1.659600   43851044   \n",
      "3             NaN              Andorra  42.506300   1.521800      77265   \n",
      "4             NaN               Angola -11.202700  17.873900   29780000   \n",
      "5             NaN  Antigua and Barbuda  17.060800 -61.796400      97929   \n",
      "6             NaN            Argentina -38.416100 -63.616700   45195774   \n",
      "7             NaN              Armenia  40.069100  45.038200    2963243   \n",
      "8             NaN              Austria  47.516200  14.550100    9006398   \n",
      "9             NaN           Azerbaijan  40.143100  47.576900   10139177   \n",
      "10            NaN              Bahrain  26.027500  50.550000    1701575   \n",
      "11            NaN           Bangladesh  23.685000  90.356300  164689383   \n",
      "12            NaN             Barbados  13.193900 -59.543200     287375   \n",
      "13            NaN              Belarus  53.709800  27.953400    9449323   \n",
      "14            NaN              Belgium  50.833300   4.000000   11589623   \n",
      "\n",
      "    urbanpop  density  medianage         smokers  tests     testpop  \\\n",
      "0    9732086       60         18     -999.000000   -999 -999.000000   \n",
      "1    1813012      105         36   846072.318000   -999 -999.000000   \n",
      "2   32011262       18         29  6840762.864000   -999 -999.000000   \n",
      "3      67993      164         45    25111.125000   -999 -999.000000   \n",
      "4       -999     -999       -999     -999.000000   -999 -999.000000   \n",
      "5      25461      223         34     -999.000000   -999 -999.000000   \n",
      "6   42032069       17         32 10824387.870000   -999 -999.000000   \n",
      "7    1866843      104         35   797112.367000    694 4269.802594   \n",
      "8    5133646      109         43  3165748.897000  10278  876.279237   \n",
      "9    5677939      123         32  2377637.006000   -999 -999.000000   \n",
      "10   1514401     2239         32   231414.200000  13553  125.549694   \n",
      "11  64228859     1265         28 33349600.060000   -999 -999.000000   \n",
      "12     89086      668         40    20116.250000   -999 -999.000000   \n",
      "13   7464965       47         40  2683607.732000  16000  590.582688   \n",
      "14  11357830      383         42  2694587.348000   4225 2743.106036   \n",
      "\n",
      "    health_exp_pc    hospibed  \n",
      "0       60.000000    0.500000  \n",
      "1      266.000000    2.900000  \n",
      "2      292.000000    1.900000  \n",
      "3     4316.000000    2.500000  \n",
      "4      109.000000 -999.000000  \n",
      "5      657.000000    3.800000  \n",
      "6      998.000000    5.000000  \n",
      "7      366.000000    4.200000  \n",
      "8     4536.000000    7.600000  \n",
      "9      368.000000    4.700000  \n",
      "10    1190.000000    6.800000  \n",
      "11      32.000000    0.800000  \n",
      "12    1160.000000    5.800000  \n",
      "13     352.000000   11.000000  \n",
      "14    4228.000000    6.200000  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5 - Merge the data frame using Fuzzy logic",
   "id": "4e6a078344adaeb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:06.479466Z",
     "start_time": "2025-11-13T06:22:03.563374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# --- Step 1: Prepare for Fuzzy Match (Cartesian Product) ---\n",
    "# Create a temporary DataFrame of all possible matches by merging on a dummy key.\n",
    "# This assumes the combined size is manageable (e.g., L_rows * R_rows < 1,000,000).\n",
    "df_master['_key'] = 0\n",
    "df_information['_key'] = 0\n",
    "df_cartesian = pd.merge(df_master, df_information, on='_key').drop('_key', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 2: Calculate Similarity Score ---\n",
    "def get_fuzzy_score(row):\n",
    "    \"\"\"Calculates the partial ratio score between the two product names.\"\"\"\n",
    "    # Using token_set_ratio is often better for dealing with word order/punctuation\n",
    "    return fuzz.token_set_ratio(row['Country/Region'], row['Entity'])\n",
    "\n",
    "# Apply the function to create a new column with the score\n",
    "df_cartesian['score'] = df_cartesian.apply(get_fuzzy_score, axis=1)\n",
    "\n",
    "\n",
    "# --- Step 3: Filter and Select the Best Match ---\n",
    "# Define a minimum threshold for a match\n",
    "THRESHOLD = 80\n",
    "\n",
    "# Filter out rows below the threshold\n",
    "df_matches = df_cartesian[df_cartesian['score'] >= THRESHOLD]\n",
    "\n",
    "# For each left product, keep only the right product with the highest score\n",
    "idx = df_matches.groupby(['Country/Region'])['score'].transform(\"max\") == df_matches['score']\n",
    "df_best_matches = df_matches[idx]\n",
    "print(df_best_matches)\n",
    "df_best_matches.loc[:,'temperature'] = \"\"\n",
    "df_best_matches.loc[:,'feels_like'] = \"\"\n",
    "df_best_matches.loc[:,'humidity'] = \"\"\n",
    "\n",
    "df_best_matches.drop_duplicates().to_csv('output_data2.csv', index=False)\n"
   ],
   "id": "77921240810db720",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m fuzz.token_set_ratio(row[\u001B[33m'\u001B[39m\u001B[33mCountry/Region\u001B[39m\u001B[33m'\u001B[39m], row[\u001B[33m'\u001B[39m\u001B[33mEntity\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Apply the function to create a new column with the score\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m df_cartesian[\u001B[33m'\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf_cartesian\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_fuzzy_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# --- Step 3: Filter and Select the Best Match ---\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# Define a minimum threshold for a match\u001B[39;00m\n\u001B[32m     24\u001B[39m THRESHOLD = \u001B[32m80\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/frame.py:10381\u001B[39m, in \u001B[36mDataFrame.apply\u001B[39m\u001B[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[39m\n\u001B[32m  10367\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapply\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[32m  10369\u001B[39m op = frame_apply(\n\u001B[32m  10370\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m  10371\u001B[39m     func=func,\n\u001B[32m   (...)\u001B[39m\u001B[32m  10379\u001B[39m     kwargs=kwargs,\n\u001B[32m  10380\u001B[39m )\n\u001B[32m> \u001B[39m\u001B[32m10381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mapply\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/apply.py:916\u001B[39m, in \u001B[36mFrameApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_raw(engine=\u001B[38;5;28mself\u001B[39m.engine, engine_kwargs=\u001B[38;5;28mself\u001B[39m.engine_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1063\u001B[39m, in \u001B[36mFrameApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1061\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1062\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.engine == \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m         results, res_index = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1064\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m         results, res_index = \u001B[38;5;28mself\u001B[39m.apply_series_numba()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1081\u001B[39m, in \u001B[36mFrameApply.apply_series_generator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1078\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[33m\"\u001B[39m\u001B[33mmode.chained_assignment\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1079\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[32m   1080\u001B[39m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1081\u001B[39m         results[i] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1082\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[32m   1083\u001B[39m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[32m   1084\u001B[39m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[32m   1085\u001B[39m             results[i] = results[i].copy(deep=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mget_fuzzy_score\u001B[39m\u001B[34m(row)\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Calculates the partial ratio score between the two product names.\"\"\"\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# Using token_set_ratio is often better for dealing with word order/punctuation\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fuzz.token_set_ratio(row[\u001B[33m'\u001B[39m\u001B[33mCountry/Region\u001B[39m\u001B[33m'\u001B[39m], \u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mEntity\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/series.py:1130\u001B[39m, in \u001B[36mSeries.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1127\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[key]\n\u001B[32m   1129\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[32m-> \u001B[39m\u001B[32m1130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[32m   1133\u001B[39m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[32m   1134\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/series.py:1246\u001B[39m, in \u001B[36mSeries._get_value\u001B[39m\u001B[34m(self, label, takeable)\u001B[39m\n\u001B[32m   1243\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[label]\n\u001B[32m   1245\u001B[39m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1246\u001B[39m loc = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[32m   1249\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[loc]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3810\u001B[39m casted_key = \u001B[38;5;28mself\u001B[39m._maybe_cast_indexer(key)\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:22:06.498121Z",
     "start_time": "2025-11-13T06:19:34.834588Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "29f3894d259e69e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* What changes were made to the data?\n",
    "\n",
    "The data was modified to include data for larger duration which was earlier 62 days. Now it contains data from 01/2020 - 09/2025. This will help show us the effect of weather on the Covid trackers properly. For the countries which are bigger like (Canada, China, USA), a separate statewise analysis is required and wont be accurate because the weather is very different throughout of the size. For sake of data we just have considered the National Capital of such countries.\n",
    "\n",
    "\n",
    "* Are there any legal or regulatory guidelines for your data or project topic?\n",
    "\n",
    "Since the data doesn't contain the private Health info it doesn't violate and personal Health privacy violations, though ethics and reasearch guidelines should be maintained. Transparency and Mitigation of Algorithmic Bias to ensure the models and findings do not unfairly impact certain communities.\n",
    "\n",
    "\n",
    "\n",
    "* What risks could be created based on the transformations done?\n",
    "\n",
    "Duplication of data might happen in case of joining multiple dataframes, Fuzzy algorithm threshold has to be watched in order to match the appropriate keys.\n",
    "\n",
    "\n",
    "* Did you make any assumptions in cleaning/transforming the data?\n",
    "For bigger countires the weather linking might not work until looked at state level. For big countries the analysis should be handled separately\n",
    "\n",
    "\n",
    "* How was your data sourced/verified for credibility?\n",
    "\n",
    "My response capability is based on the information I was trained on (a massive dataset of text and code) and, for current or specific inquiries, the information I can retrieve using my Google Search tool. I do not have a private, project-specific dataset of COVID-19 that I have sourced or verified.\n",
    "However I am relying on https://ourworldindata.org which sources data from Authentic sources\n",
    "\n",
    "\n",
    "* Was your data acquired in an ethical way?\n",
    "\n",
    "It was an open source data for public use.\n",
    "\n",
    "\n",
    "* How would you mitigate any of the ethical implications you have identified?\n",
    "\n",
    "I'd mitigate ethical risks by focusing on two areas: Privacy and Fairness.\n",
    "\n",
    "For Privacy, I'd use aggressive data aggregation (county-level, weekly counts) and implement Data Use Agreements (DUAs) to prohibit re-identification attempts. Data would be stored in a secure, encrypted data enclave.\n",
    "\n",
    "For Fairness, I'd explicitly include and control for socioeconomic factors in the analysis to prevent weather correlations from masking underlying health disparities. All model results would be reported with clear limitations and a focus on actionable public health policy, not stigmatization."
   ],
   "id": "8cf93c44b594e5b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
